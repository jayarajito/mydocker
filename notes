Docker is an open platform for developing, shipping, and running applications.

Docker allows you to separate your applications from your infrastructure so you can deliver software quickly. 

Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.

By taking advantage of Docker's methodologies for shipping, testing, and deploying code, 
you can significantly reduce the delay between writing code and running it in production.

Docker provides the ability to package and run an application in a loosely isolated environment called a container. 
The isolation and security lets you run many containers simultaneously on a given host. 
Containers are lightweight and contain everything needed to run the application, so you don't need to rely on what's installed on the host. 

Containers are great for continuous integration and continuous delivery (CI/CD) workflows.

Consider the following example scenario:

Your developers write code locally and share their work with their colleagues using Docker containers.
They use Docker to push their applications into a test environment and run automated and manual tests.
When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation.
When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.

Docker architecture
Docker uses a client-server architecture. The Docker client talks to the Docker daemon, 
which does the heavy lifting of building, running, and distributing your Docker containers. 
The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. 
The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. 
Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.

The Docker daemon
The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. 
A daemon can also communicate with other daemons to manage Docker services.

The Docker client
The Docker client (docker) is the primary way that many Docker users interact with Docker. 
When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. 
The docker command uses the Docker API. The Docker client can communicate with more than one daemon.

Docker Desktop
Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables 
you to build and share containerized applications and microservices. 
Docker Desktop includes the Docker daemon (dockerd), the Docker client (docker), Docker Compose, 
Docker Content Trust, Kubernetes, and Credential Helper.

Docker registries
A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, 
and Docker looks for images on Docker Hub by default. You can even run your own private registry.

When you use the docker pull or docker run commands, Docker pulls the required images from your 
configured registry. When you use the docker push command, Docker pushes your image to your configured registry.

Docker objects
When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. 
This section is a brief overview of some of those objects.

Images
An image is a read-only template with instructions for creating a Docker container. 
Often, an image is based on another image, with some additional customization. 
For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and 
your application, as well as the configuration details needed to make your application run.

You might create your own images or you might only use those created by others and published in a registry. 
To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed 
to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. 
When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. 
This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.

Containers
A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using 
the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a 
new image based on its current state.

By default, a container is relatively well isolated from other containers and its host machine. 
You can control how isolated a container's network, storage, or other underlying subsystems are from 
other containers or from the host machine.

A container is defined by its image as well as any configuration options you provide to it 
when you create or start it. When a container is removed, any changes to its state that aren't stored 
in persistent storage disappear.

Example docker run command
The following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash.

docker run -i -t ubuntu /bin/bash

When you run this command, the following happens (assuming you are using the default registry configuration):

If you don't have the ubuntu image locally, Docker pulls it from your configured registry, as though you had 
run docker pull ubuntu manually.

Docker creates a new container, as though you had run a docker container create command manually.

Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to 
create or modify files and directories in its local filesystem.

Docker creates a network interface to connect the container to the default network, since you didn't specify any networking options. 
This includes assigning an IP address to the container. By default, containers can connect to external networks using 
the host machine's network connection.

Docker starts the container and executes /bin/bash. Because the container is running interactively and attached to your 
terminal (due to the -i and -t flags), you can provide input using your keyboard while Docker logs the output to your terminal.

When you run exit to terminate the /bin/bash command, the container stops but isn't removed. You can start it again or remove it.


The underlying technology
Docker is written in the Go programming language and takes advantage of several features of the Linux kernel to deliver its 
functionality. 
Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, 
Docker creates a set of namespaces for that container.

These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is 
limited to that namespace.

Run your first container

docker run -d -p 8080:80 docker/welcome-to-docker

Start the project

git clone https://github.com/docker/getting-started-todo-app

cd getting-started-todo-app

docker compose watch

netstat -ano | findstr :8080
taskkill /PID 2660 /F

Pushing images
You can push a new image to this repository using the CLI:

docker tag local-image:tagname new-repo:tagname
docker push new-repo:tagname
Make sure to replace tagname with your desired image repository tag.

To push a new tag to this repository:

docker push jayarajdevops/getting-started-todo-app:tagname


A Dockerfile is a text-based script that provides the instruction set on how to build the image. 

docker build -t <DOCKER_USERNAME>/getting-started-todo-app .

docker build -t <DOCKER_USERNAME>/getting-started-todo-app .

docker image ls

Docker provides a single command that will clean up any resources — images, containers, volumes, and networks — that are dangling (not tagged or associated with a container):

docker system prune

To additionally remove any stopped containers and all unused images (not just dangling images), add the -a flag to the command:

docker system prune -a


A Dockerfile describes how to build a Docker image, while Docker Compose is a tool that runs Docker containers. 
Dockerfile
A text document that contains commands to build a Docker image 
Each instruction in a Dockerfile creates a layer 
Dockerfiles can be used to build Docker images with detailed customization options 
Docker Compose 
A tool that defines and runs multi-container Docker applications
Uses a YAML file to configure an application's services
Streamlines the management of multi-container applications
How they work together 
A Docker-compose.yml file can specify a build context that includes a Dockerfile
This allows Docker Compose to build the image as part of the service's configuration
Examples of Dockerfile instructions 
FROM: Creates a layer from a Docker image
COPY: Adds files from the Docker client's current directory
RUN: Builds an application with make
CMD: Specifies what command to run within the container


A Dockerfile is a text-based script that provides the instruction set on how to build the image.

git clone https://github.com/docker/getting-started-todo-app

cd getting-started-todo-app


Build the project by running the following command, swapping out DOCKER_USERNAME with your username.

docker build -t <DOCKER_USERNAME>/getting-started-todo-app .

To verify the image exists locally, you can use the docker image ls command:

docker image ls

To push the image, use the docker push command. Be sure to replace DOCKER_USERNAME with your username:

docker push <DOCKER_USERNAME>/getting-started-todo-app

docker run -d -p 8080:80 docker/welcome-to-docker


What is a container?
Simply put, containers are isolated processes for each of your app's components. Each component - the frontend React app, the Python API engine, and the database - runs in its own isolated environment, completely isolated from everything else on your machine.

Here's what makes them awesome. Containers are:

Self-contained. Each container has everything it needs to function with no reliance on any pre-installed dependencies on the host machine.
Isolated. Since containers are run in isolation, they have minimal influence on the host and other containers, increasing the security of your applications.
Independent. Each container is independently managed. Deleting one container won't affect any others.
Portable. Containers can run anywhere! The container that runs on your development machine will work the same way in a data center or anywhere in the cloud!
Containers versus virtual machines (VMs)
Without getting too deep, a VM is an entire operating system with its own kernel, hardware drivers, programs, and applications. Spinning up a VM only to isolate a single application is a lot of overhead.

A container is simply an isolated process with all of the files it needs to run. If you run multiple containers, they all share the same kernel, allowing you to run more applications on less infrastructure.





Docker Welcome Example
https://github.com/docker/welcome-to-docker/blob/main/Dockerfile

# Start your image with a node base image
FROM node:18-alpine

# The /app directory should act as the main application directory
WORKDIR /app

# Copy the app package and package-lock.json file
COPY package*.json ./

# Copy local directories to the current local directory of our docker image (/app)
COPY ./src ./src
COPY ./public ./public

# Install node packages, install serve, build the app, and remove dependencies at the end
RUN npm install \
    && npm install -g serve \
    && npm run build \
    && rm -fr node_modules

EXPOSE 3000

# Start the app using serve command
CMD [ "serve", "-s", "build" ]


docker run -d -p 8088:80 --name welcome-to-docker docker/welcome-to-docker
docker run -d -p 8088:3000 --name mywebapp  mydockerlocal:v1.1

Open http://localhost:8088 in your browser to see the app running.

docker exec -it <container_name> sh

The '&&' command ensures that the second command ( cd new_directory ) only runs if the first command ( mkdir new_directory ) executes successfully.

docker ps
The docker ps command will show you only running containers. To view stopped containers, add the -a flag to list all containers: docker ps -a

Stop container

docker stop <the-container-id>

When referencing containers by ID, you don't need to provide the full ID. You only need to provide enough of the ID to make it unique.
docker stop a1f

What is an image?

A container image is a standardized package that includes all of the files, binaries, libraries, and configurations to run a container.

There are two important principles of images:

Images are immutable. Once an image is created, it can't be modified. You can only make a new image or add changes on top of it.

Container images are composed of layers. Each layer represents a set of file system changes that add, remove, or modify files.

Open a terminal and search for images using the docker search command:

docker search docker/welcome-to-docker

Pull the image using the docker pull command.

docker pull docker/welcome-to-docker

Each of line represents a different downloaded layer of the image. Remember that each layer is a set of filesystem changes and provides functionality of the image.
List your downloaded images using the docker image ls command:

docker image ls

List the image's layers using the docker image history command:

docker image history docker/welcome-to-docker

Viewing the full command

If you add the --no-trunc flag to the command, you will see the full command. Note that, since the output is in a table-like format, longer commands will cause the output to be very difficult to navigate.

What is a registry?
An image registry is a centralized location for storing and sharing your container images. It can be either public or private. Docker Hub is a public registry that anyone can use and is the default registry.

While Docker Hub is a popular option, there are many other available container registries available today, including Amazon Elastic Container Registry(ECR), Azure Container Registry (ACR), and Google Container Registry (GCR). You can even run your private registry on your local system or inside your organization. For example, Harbor, JFrog Artifactory, GitLab Container registry etc.

Registry vs. repository

A registry is a centralized location that stores and manages container images, whereas a repository is a collection of related container images within a registry. Think of it as a folder where you organize your images based on projects. Each repository contains one or more container images.

You can create one private repository and unlimited public repositories using the free version of Docker Hub.

Sample

Clone the GitHub repository using the following command:

git clone https://github.com/dockersamples/helloworld-demo-node

Navigate into the newly created directory.

cd helloworld-demo-node

Run the following command to build a Docker image, swapping out YOUR_DOCKER_USERNAME with your username.

docker build -t <YOUR_DOCKER_USERNAME>/docker-quickstart .

Make sure you include the dot (.) at the end of the docker build command. This tells Docker where to find the Dockerfile.

Run the following command to list the newly created Docker image:

docker images

Start a container to test the image by running the following command (swap out the username with your own username):

docker run -d -p 8080:8080 <YOUR_DOCKER_USERNAME>/docker-quickstart 


Use the docker tag command to tag the Docker image. Docker tags allow you to label and version your images.

docker tag <YOUR_DOCKER_USERNAME>/docker-quickstart <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0 

Finally, it's time to push the newly built image to your Docker Hub repository by using the docker push command:

docker push <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0

docker tag mydockerlocal:v1.1 jayarajdevops/mydockerlocal:webver

What is Docker Compose?

If you've been following the guides so far, you've been working with single container applications. But, now you're wanting to do something more complicated - run databases, message queues, caches, or a variety of other services. Do you install everything in a single container? Run multiple containers? If you run multiple, how do you connect them all together?

You can use multiple docker run commands to start multiple containers. But, you'll soon realize you'll need to manage networks, all of the flags needed to connect containers to those networks, and more. And when you're done, cleanup is a little more complicated.

With Docker Compose, you can define all of your containers and their configurations in a single YAML file. If you include this file in your code repository, anyone that clones your repository can get up and running with a single command.


It's important to understand that Compose is a declarative tool - you simply define it and go. You don't always need to recreate everything from scratch. If you make a change, run docker compose up again and Compose will reconcile the changes in your file and apply them intelligently.

Dockerfile versus Compose file
A Dockerfile provides instructions to build a container image while a Compose file defines your running containers. 
Quite often, a Compose file references a Dockerfile to build an image to use for a particular service.

Try it out
In this hands-on, you will learn how to use a Docker Compose to run a multi-container application. 
You'll use a simple to-do list app built with Node.js and MySQL as a database server.

https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-docker-compose/

Follow the instructions to run the to-do list app on your system.

git clone https://github.com/dockersamples/todo-list-app 

Navigate into the todo-list-app directory:

cd todo-list-app

Inside this directory, you'll find a file named compose.yaml. This YAML file is where all the magic happens! It defines all the services that make up your application, along with their configurations. 
Each service specifies its image, ports, volumes, networks, and any other settings necessary for its functionality. 
Take some time to explore the YAML file and familiarize yourself with its structure.


Use the docker compose up command to start the application:

docker compose up -d --build

Two container images were downloaded from Docker Hub - node and MySQL
A network was created for your application
A volume was created to persist the database files between container restarts
Two containers were started with all of their necessary config

A "volume" refers to a single, accessible storage area on a computer, often with its own file system, while a "partition" is a logical division of a physical disk drive, meaning it's a distinct section of storage space on a single hard drive;

Key points to remember:
Partition:
A physical division of a hard disk, creating separate areas for data storage. 
Volume:
A logical grouping of partitions, which the operating system sees as a single accessible storage space with a specific file system. 
Example: Imagine a hard drive with three partitions; you could then create a single "volume" by combining two of those partitions, allowing the operating system to access them as one unified storage area. 

A file system is a way to organize and store files on a storage device like a hard drive, USB flash drive, or solid state drive. It defines how data is accessed, organized, and stored. 
How file systems work
Different file systems have different characteristics. 
Some file systems are specific to certain operating systems or devices. 
The structure of a file system includes how files and directories are organized and stored on the physical storage device. 
Examples of file systems

FAT
One of the oldest and simplest file systems, used in many removable storage devices. 
NTFS
The default file system for Windows NT-based operating systems, including Windows 11. It supports large files and partitions. 

HFS
Also known as Mac OS Standard, it was developed for use with Mac operating systems. 

ext4
The default file system for most Linux distributions. It's used in various applications, including desktops, servers, and data centers. 
File system use cases 
Storing information about users, software applications, or other data that are needed for running programs
Storing pictures, videos, music, and other types of media

With everything now up and running, you can open http://localhost:3000 in your browser to see the site. Feel free to add items to the list, check them off, and remove them.


Tear it down

In the CLI, use the docker compose down command to remove everything:

docker compose down

Volume persistence

By default, volumes aren't automatically removed when you tear down a Compose stack. The idea is that you might want the data back if you start the stack again.

If you do want to remove the volumes, add the --volumes flag when running the docker compose down command:


docker compose down --volumes


Using the GUI for Compose stacks

Note that if you remove the containers for a Compose app in the GUI, it's removing only the containers. You'll have to manually remove the network and volumes if you want to do so.

https://docs.docker.com/get-started/docker-concepts/building-images/

Stacking the layers
Layering is made possible by content-addressable storage and union filesystems. While this will get technical, here’s how it works:

After each layer is downloaded, it is extracted into its own directory on the host filesystem.
When you run a container from an image, a union filesystem is created where layers are stacked on top of each other, creating a new and unified view.
When the container starts, its root directory is set to the location of this unified directory, using chroot.

When the union filesystem is created, in addition to the image layers, a directory is created specifically for the running container. 
This allows the container to make filesystem changes while allowing the original image layers to remain untouched. This enables you to run multiple containers from the same underlying image.

create new image layers manually using the command.

docker container commit

https://docs.docker.com/reference/cli/docker/container/commit/

In a terminal, run the following command to start a new container:

docker run --name=base-container -ti ubuntu

Inside the container, run the following command to install Node.js:

apt update && apt install -y nodejs

When this command runs, it downloads and installs Node inside the container. 
In the context of the union filesystem, these filesystem changes occur within the directory unique to this container.

Validate if Node is installed by running the following command:

node -e 'console.log("Hello world!")'


Now that you have Node installed, you’re ready to save the changes you’ve made as a new image layer, from which you can start new containers 
or build new images. To do so, you will use the docker container commit command. Run the following command in a new terminal:

docker container commit -m "Add node" base-container node-base

View the layers of your image using the docker image history command:

docker image history node-base

To prove your image has Node installed, you can start a new container using this new image:

docker run node-base node -e "console.log('Hello again')"


Now that you’re done creating your base image, you can remove that container:
docker rm -f base-container

Build an app image

Now that you have a base image, you can extend that image to build additional images.

Start a new container using the newly created node-base image:

docker run --name=app-container -ti node-base

Inside of this container, run the following command to create a Node program:

echo 'console.log("Hello from an app")' > app.js


To run this Node program, you can use the following command and see the message printed on the screen:

node app.js

In another terminal, run the following command to save this container’s changes as a new image:

docker container commit -c "CMD node app.js" -m "Add app" app-container sample-app

This command not only creates a new image named sample-app, but also adds additional configuration to the image to set the default 
command when starting a container. In this case, you are setting it to automatically run node app.js.

In a terminal outside of the container, run the following command to view the updated layers:

docker image history sample-app

Finally, start a new container using the brand new image. Since you specified the default command, you can use the following command:

docker run sample-app

Now that you’re done with your containers, you can remove them using the following command:

docker rm -f app-container


most image builds don’t use docker container commit. Instead, you’ll use a Dockerfile which automates these steps for you.

Writing a Dockerfile
https://docs.docker.com/get-started/docker-concepts/building-images/writing-a-dockerfile/

A Dockerfile is a text-based document that's used to create a container image. 
It provides instructions to the image builder on the commands to run, files to copy, startup command, and more.

As an example, the following Dockerfile would produce a ready-to-run Python application:

FROM python:3.12
WORKDIR /usr/local/app

# Install the application dependencies
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# Copy in the source code
COPY src ./src
EXPOSE 5000

# Setup an app user so the container doesn't run as the root user
RUN useradd app
USER app

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8080"]

Common instructions
Some of the most common instructions in a Dockerfile include:

FROM <image> - this specifies the base image that the build will extend.
WORKDIR <path> - this instruction specifies the "working directory" or the path in the image where files will be copied and commands will be executed.
COPY <host-path> <image-path> - this instruction tells the builder to copy files from the host and put them into the container image.
RUN <command> - this instruction tells the builder to run the specified command.
ENV <name> <value> - this instruction sets an environment variable that a running container will use.
EXPOSE <port-number> - this instruction sets configuration on the image that indicates a port the image would like to expose.
USER <user-or-uid> - this instruction sets the default user for all subsequent instructions.
CMD ["<command>", "<arg1>"] - this instruction sets the default command a container using this image will run.
To read through all of the instructions or go into greater detail, check out the Dockerfile reference.

https://docs.docker.com/reference/dockerfile/


Just as you saw with the previous example, a Dockerfile typically follows these steps:

Determine your base image
Install application dependencies
Copy in any relevant source code and/or binaries
Configure the final image

Dockerfile file extensions

It's important to note that the Dockerfile has no file extension. 

This Dockerfile isn't production-ready yet

It's important to note that this Dockerfile is not following all of the best practices yet (by design). It will build the app, but the builds won't be as fast, or the images as secure, as they could be.

Keep reading to learn more about how to make the image maximize the build cache, run as a non-root user, and multi-stage builds.


Containerize new projects quickly with docker init

The docker init command will analyze your project and quickly create a Dockerfile, a compose.yaml, and a .dockerignore, helping you get up and going. Since you're learning about Dockerfiles specifically here, you won't use it now. But, learn more about it here.

Build, tag, and publish an image
https://docs.docker.com/get-started/docker-concepts/building-images/build-tag-and-publish-an-image/
