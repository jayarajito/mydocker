Docker is an open platform for developing, shipping, and running applications.

Docker allows you to separate your applications from your infrastructure so you can deliver software quickly. 

Commercial use of Docker Desktop in larger enterprises (more than 250 employees OR more than $10 million USD in annual revenue) requires a paid subscription.

By taking advantage of Docker's methodologies for shipping, testing, and deploying code, 
you can significantly reduce the delay between writing code and running it in production.

Docker provides the ability to package and run an application in a loosely isolated environment called a container. 
The isolation and security lets you run many containers simultaneously on a given host. 
Containers are lightweight and contain everything needed to run the application, so you don't need to rely on what's installed on the host. 

Containers are great for continuous integration and continuous delivery (CI/CD) workflows.

Consider the following example scenario:

Your developers write code locally and share their work with their colleagues using Docker containers.
They use Docker to push their applications into a test environment and run automated and manual tests.
When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation.
When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.

Docker architecture
Docker uses a client-server architecture. The Docker client talks to the Docker daemon, 
which does the heavy lifting of building, running, and distributing your Docker containers. 
The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. 
The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. 
Another Docker client is Docker Compose, that lets you work with applications consisting of a set of containers.

The Docker daemon
The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. 
A daemon can also communicate with other daemons to manage Docker services.

The Docker client
The Docker client (docker) is the primary way that many Docker users interact with Docker. 
When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. 
The docker command uses the Docker API. The Docker client can communicate with more than one daemon.

Docker Desktop
Docker Desktop is an easy-to-install application for your Mac, Windows or Linux environment that enables 
you to build and share containerized applications and microservices. 
Docker Desktop includes the Docker daemon (dockerd), the Docker client (docker), Docker Compose, 
Docker Content Trust, Kubernetes, and Credential Helper.

Docker registries
A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, 
and Docker looks for images on Docker Hub by default. You can even run your own private registry.

When you use the docker pull or docker run commands, Docker pulls the required images from your 
configured registry. When you use the docker push command, Docker pushes your image to your configured registry.

Docker objects
When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. 
This section is a brief overview of some of those objects.

Images
An image is a read-only template with instructions for creating a Docker container. 
Often, an image is based on another image, with some additional customization. 
For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and 
your application, as well as the configuration details needed to make your application run.

You might create your own images or you might only use those created by others and published in a registry. 
To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed 
to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. 
When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. 
This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.

Containers
A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using 
the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a 
new image based on its current state.

By default, a container is relatively well isolated from other containers and its host machine. 
You can control how isolated a container's network, storage, or other underlying subsystems are from 
other containers or from the host machine.

A container is defined by its image as well as any configuration options you provide to it 
when you create or start it. When a container is removed, any changes to its state that aren't stored 
in persistent storage disappear.

Example docker run command
The following command runs an ubuntu container, attaches interactively to your local command-line session, and runs /bin/bash.

docker run -i -t ubuntu /bin/bash

When you run this command, the following happens (assuming you are using the default registry configuration):

If you don't have the ubuntu image locally, Docker pulls it from your configured registry, as though you had 
run docker pull ubuntu manually.

Docker creates a new container, as though you had run a docker container create command manually.

Docker allocates a read-write filesystem to the container, as its final layer. This allows a running container to 
create or modify files and directories in its local filesystem.

Docker creates a network interface to connect the container to the default network, since you didn't specify any networking options. 
This includes assigning an IP address to the container. By default, containers can connect to external networks using 
the host machine's network connection.

Docker starts the container and executes /bin/bash. Because the container is running interactively and attached to your 
terminal (due to the -i and -t flags), you can provide input using your keyboard while Docker logs the output to your terminal.

When you run exit to terminate the /bin/bash command, the container stops but isn't removed. You can start it again or remove it.


The underlying technology
Docker is written in the Go programming language and takes advantage of several features of the Linux kernel to deliver its 
functionality. 
Docker uses a technology called namespaces to provide the isolated workspace called the container. When you run a container, 
Docker creates a set of namespaces for that container.

These namespaces provide a layer of isolation. Each aspect of a container runs in a separate namespace and its access is 
limited to that namespace.

Run your first container

docker run -d -p 8080:80 docker/welcome-to-docker

Start the project

git clone https://github.com/docker/getting-started-todo-app

cd getting-started-todo-app

docker compose watch

netstat -ano | findstr :8080
taskkill /PID 2660 /F

Pushing images
You can push a new image to this repository using the CLI:

docker tag local-image:tagname new-repo:tagname
docker push new-repo:tagname
Make sure to replace tagname with your desired image repository tag.

To push a new tag to this repository:

docker push jayarajdevops/getting-started-todo-app:tagname


A Dockerfile is a text-based script that provides the instruction set on how to build the image. 

docker build -t <DOCKER_USERNAME>/getting-started-todo-app .

docker build -t <DOCKER_USERNAME>/getting-started-todo-app .

docker image ls

Docker provides a single command that will clean up any resources — images, containers, volumes, and networks — that are dangling (not tagged or associated with a container):

docker system prune

To additionally remove any stopped containers and all unused images (not just dangling images), add the -a flag to the command:

docker system prune -a


A Dockerfile describes how to build a Docker image, while Docker Compose is a tool that runs Docker containers. 
Dockerfile
A text document that contains commands to build a Docker image 
Each instruction in a Dockerfile creates a layer 
Dockerfiles can be used to build Docker images with detailed customization options 
Docker Compose 
A tool that defines and runs multi-container Docker applications
Uses a YAML file to configure an application's services
Streamlines the management of multi-container applications
How they work together 
A Docker-compose.yml file can specify a build context that includes a Dockerfile
This allows Docker Compose to build the image as part of the service's configuration
Examples of Dockerfile instructions 
FROM: Creates a layer from a Docker image
COPY: Adds files from the Docker client's current directory
RUN: Builds an application with make
CMD: Specifies what command to run within the container


A Dockerfile is a text-based script that provides the instruction set on how to build the image.

git clone https://github.com/docker/getting-started-todo-app

cd getting-started-todo-app


Build the project by running the following command, swapping out DOCKER_USERNAME with your username.

docker build -t <DOCKER_USERNAME>/getting-started-todo-app .

To verify the image exists locally, you can use the docker image ls command:

docker image ls

To push the image, use the docker push command. Be sure to replace DOCKER_USERNAME with your username:

docker push <DOCKER_USERNAME>/getting-started-todo-app

docker run -d -p 8080:80 docker/welcome-to-docker


What is a container?
Simply put, containers are isolated processes for each of your app's components. Each component - the frontend React app, the Python API engine, and the database - runs in its own isolated environment, completely isolated from everything else on your machine.

Here's what makes them awesome. Containers are:

Self-contained. Each container has everything it needs to function with no reliance on any pre-installed dependencies on the host machine.
Isolated. Since containers are run in isolation, they have minimal influence on the host and other containers, increasing the security of your applications.
Independent. Each container is independently managed. Deleting one container won't affect any others.
Portable. Containers can run anywhere! The container that runs on your development machine will work the same way in a data center or anywhere in the cloud!
Containers versus virtual machines (VMs)
Without getting too deep, a VM is an entire operating system with its own kernel, hardware drivers, programs, and applications. Spinning up a VM only to isolate a single application is a lot of overhead.

A container is simply an isolated process with all of the files it needs to run. If you run multiple containers, they all share the same kernel, allowing you to run more applications on less infrastructure.





Docker Welcome Example
https://github.com/docker/welcome-to-docker/blob/main/Dockerfile

# Start your image with a node base image
FROM node:18-alpine

# The /app directory should act as the main application directory
WORKDIR /app

# Copy the app package and package-lock.json file
COPY package*.json ./

# Copy local directories to the current local directory of our docker image (/app)
COPY ./src ./src
COPY ./public ./public

# Install node packages, install serve, build the app, and remove dependencies at the end
RUN npm install \
    && npm install -g serve \
    && npm run build \
    && rm -fr node_modules

EXPOSE 3000

# Start the app using serve command
CMD [ "serve", "-s", "build" ]


docker run -d -p 8088:80 --name welcome-to-docker docker/welcome-to-docker
docker run -d -p 8088:3000 --name mywebapp  mydockerlocal:v1.1

Open http://localhost:8088 in your browser to see the app running.

docker exec -it <container_name> sh

The '&&' command ensures that the second command ( cd new_directory ) only runs if the first command ( mkdir new_directory ) executes successfully.

docker ps
The docker ps command will show you only running containers. To view stopped containers, add the -a flag to list all containers: docker ps -a

Stop container

docker stop <the-container-id>

When referencing containers by ID, you don't need to provide the full ID. You only need to provide enough of the ID to make it unique.
docker stop a1f

What is an image?

A container image is a standardized package that includes all of the files, binaries, libraries, and configurations to run a container.

There are two important principles of images:

Images are immutable. Once an image is created, it can't be modified. You can only make a new image or add changes on top of it.

Container images are composed of layers. Each layer represents a set of file system changes that add, remove, or modify files.

Open a terminal and search for images using the docker search command:

docker search docker/welcome-to-docker

Pull the image using the docker pull command.

docker pull docker/welcome-to-docker

Each of line represents a different downloaded layer of the image. Remember that each layer is a set of filesystem changes and provides functionality of the image.
List your downloaded images using the docker image ls command:

docker image ls

List the image's layers using the docker image history command:

docker image history docker/welcome-to-docker

Viewing the full command

If you add the --no-trunc flag to the command, you will see the full command. Note that, since the output is in a table-like format, longer commands will cause the output to be very difficult to navigate.

What is a registry?
An image registry is a centralized location for storing and sharing your container images. It can be either public or private. Docker Hub is a public registry that anyone can use and is the default registry.

While Docker Hub is a popular option, there are many other available container registries available today, including Amazon Elastic Container Registry(ECR), Azure Container Registry (ACR), and Google Container Registry (GCR). You can even run your private registry on your local system or inside your organization. For example, Harbor, JFrog Artifactory, GitLab Container registry etc.

Registry vs. repository

A registry is a centralized location that stores and manages container images, whereas a repository is a collection of related container images within a registry. Think of it as a folder where you organize your images based on projects. Each repository contains one or more container images.

You can create one private repository and unlimited public repositories using the free version of Docker Hub.

Sample

Clone the GitHub repository using the following command:

git clone https://github.com/dockersamples/helloworld-demo-node

Navigate into the newly created directory.

cd helloworld-demo-node

Run the following command to build a Docker image, swapping out YOUR_DOCKER_USERNAME with your username.

docker build -t <YOUR_DOCKER_USERNAME>/docker-quickstart .

Make sure you include the dot (.) at the end of the docker build command. This tells Docker where to find the Dockerfile.

Run the following command to list the newly created Docker image:

docker images

Start a container to test the image by running the following command (swap out the username with your own username):

docker run -d -p 8080:8080 <YOUR_DOCKER_USERNAME>/docker-quickstart 


Use the docker tag command to tag the Docker image. Docker tags allow you to label and version your images.

docker tag <YOUR_DOCKER_USERNAME>/docker-quickstart <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0 

Finally, it's time to push the newly built image to your Docker Hub repository by using the docker push command:

docker push <YOUR_DOCKER_USERNAME>/docker-quickstart:1.0

docker tag mydockerlocal:v1.1 jayarajdevops/mydockerlocal:webver

What is Docker Compose?

If you've been following the guides so far, you've been working with single container applications. But, now you're wanting to do something more complicated - run databases, message queues, caches, or a variety of other services. Do you install everything in a single container? Run multiple containers? If you run multiple, how do you connect them all together?

You can use multiple docker run commands to start multiple containers. But, you'll soon realize you'll need to manage networks, all of the flags needed to connect containers to those networks, and more. And when you're done, cleanup is a little more complicated.

With Docker Compose, you can define all of your containers and their configurations in a single YAML file. If you include this file in your code repository, anyone that clones your repository can get up and running with a single command.


It's important to understand that Compose is a declarative tool - you simply define it and go. You don't always need to recreate everything from scratch. If you make a change, run docker compose up again and Compose will reconcile the changes in your file and apply them intelligently.

Dockerfile versus Compose file
A Dockerfile provides instructions to build a container image while a Compose file defines your running containers. 
Quite often, a Compose file references a Dockerfile to build an image to use for a particular service.

Try it out
In this hands-on, you will learn how to use a Docker Compose to run a multi-container application. 
You'll use a simple to-do list app built with Node.js and MySQL as a database server.

https://docs.docker.com/get-started/docker-concepts/the-basics/what-is-docker-compose/

